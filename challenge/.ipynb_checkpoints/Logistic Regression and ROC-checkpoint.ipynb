{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Lab 04 - Logistic Regression and ROC\n",
    "\n",
    "In previous labs we investigated the behavior of the linear regression model as well as of polynomial fitting using linear regression. In the latter we have also seen how the model complexity (captured by the degree $k$ of the fitted polynomial) and the noise levels impact the fitted model and its performance over a given test set.\n",
    "\n",
    "The following lab extends linear regression into a classification problem using the logistic regression model. Here, attempt to model the *probability* of a given sample $\\mathbf{x}$ to be classified as $1$. To do so, we assume that: $$ y|\\mathbf{x} \\sim Ber\\left(\\phi_\\mathbf{w}\\left(\\mathbf{x}\\right)\\right),\\quad \\phi_\\mathbf{w}\\left(\\mathbf{x}\\right) = sigm\\left(\\mathbf{x}^\\top \\mathbf{w}\\right) $$\n",
    "\n",
    "Namely, that $y$ has a Bernoulli distribution where the probability that $y=1$ depends on the model $\\mathbf{w}$ and the observation $\\mathbf{x}$ and is given by $sigm\\left(\\mathbf{x}^\\top \\mathbf{w}\\right)$. These assumptions yield the following hypothesis class of logistic regression: $$ \\mathcal{H}_{logistic}= \\left\\{\\mathbf{x}\\to sigm\\left( \\mathbf{x}^\\top\\mathbf{w}\\right) \\,\\,\\Big| \\,\\,\\mathbf{w}\\in\\mathbb{R}^{d+1}\\right\\} $$\n",
    "\n",
    "Notice that the hypothesis class defined above does not provide us with classifiers, i.e. functions that return either $0$ or $1$ but rather values in $\\left[0,1\\right]$. To turn the outputted value (which is thought of as the probability of the sample being classified as $1$) into a binary decision we must also specify a threshold value $\\alpha$. Then if the value is greater or equals to $\\alpha$ we predict the class as being $1$ and otherwise as $0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from utils import *\n",
    "from agoda_cancellation_estimator import *\n",
    "from agoda_cancellation_prediction import *\n",
    "\n",
    "np.random.seed(0)\n",
    "c = [custom[0], custom[-1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us begin with generating a two-class dataset, with the points of each class generated from some bivariate Gaussian distribution. We visualize the data using a scatter plot as well as class-specific histograms depicting the marginal distributions of samples in each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "is_executing": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13384\\1207944597.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"../datasets/agoda_cancellation_train.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mdesign_matrix\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m cancellation_labels = preprocess_labels(data.cancellation_datetime,\n",
      "\u001b[1;32m~\\OneDrive\\Documents\\CS.BSc\\SemD\\IML\\IML.HUJI repo\\challenge\\agoda_cancellation_prediction.py\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(full_data)\u001b[0m\n\u001b[0;32m     74\u001b[0m                           pd.get_dummies(full_data.charge_option,\n\u001b[0;32m     75\u001b[0m                                          drop_first=True)], axis=1)\n\u001b[1;32m---> 76\u001b[1;33m     features[\"has_cancellation_history\"] = df[\"h_customer_id\"].apply(\n\u001b[0m\u001b[0;32m     77\u001b[0m         number_of_times_cancelled)\n\u001b[0;32m     78\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "n0, n1 = 1000, 1000\n",
    "mu = np.array([[0, 0], [4, 5]])\n",
    "cov = np.array([[[1, .5], [.5, 1]], [[5, 3.5], [3.5, 8]]])\n",
    "\n",
    "data = load_data(\"../datasets/agoda_cancellation_train.csv\")\n",
    "design_matrix = preprocess(data)\n",
    "\n",
    "cancellation_labels = preprocess_labels(data.cancellation_datetime,\n",
    "                                        data.booking_datetime)\n",
    "# print(\n",
    "#     design_matrix.apply(lambda x: np.corrcoef(x.values, cancellation_labels.values)[0, 1]))\n",
    "# print(df.columns)\n",
    "# droplist = [x for x in design_matrix if np.abs(np.corrcoef(design_matrix[x], cancellation_labels)[0, 1]) < 0.01]\n",
    "# design_matrix.drop(droplist,axis=1,inplace=True)\n",
    "# print(droplist)\n",
    "# print(design_matrix)\n",
    "train_X, train_y, test_X, test_y = split_train_test(design_matrix,\n",
    "                                                    cancellation_labels)\n",
    "\n",
    "go.Figure([\n",
    "    go.Scatter(x=train_X[:, 0], y=train_X[:, 1], mode='markers',\n",
    "               marker=dict(color=train_y, colorscale=c, reversescale=True,\n",
    "                           size=3)),\n",
    "    go.Histogram(x=train_X[train_y == 0, 0], histnorm=\"probability density\",\n",
    "                 yaxis='y2', opacity=.5, marker=dict(color=c[1][1])),\n",
    "    go.Histogram(x=train_X[train_y == 1, 0], histnorm=\"probability density\",\n",
    "                 yaxis='y2', opacity=.5, marker=dict(color=c[0][1])),\n",
    "    go.Histogram(y=train_X[train_y == 0, 1], histnorm=\"probability density\",\n",
    "                 xaxis='x2', opacity=.5, marker=dict(color=c[1][1])),\n",
    "    go.Histogram(y=train_X[train_y == 1, 1], histnorm=\"probability density\",\n",
    "                 xaxis='x2', opacity=.5, marker=dict(color=c[0][1]))\n",
    "], layout=go.Layout(\n",
    "    xaxis=dict(zeroline=False, domain=[0, 0.85], showgrid=False),\n",
    "    yaxis=dict(zeroline=False, domain=[0, 0.85], showgrid=False),\n",
    "    xaxis2=dict(zeroline=False, domain=[0.85, 1], showgrid=False),\n",
    "    yaxis2=dict(zeroline=False, domain=[0.85, 1], showgrid=False),\n",
    "    hovermode='closest', showlegend=False, height=600, width=600,\n",
    "    title=r\"$(1)\\text{ Dataset To Fit Logistic Regression Model To}$\"\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fit a logistic regression model over the data generated above and predict the class assignment probability predicted by the fitted model. Therefore, for each sample we have a value expressing the probability of being assigned as class $1$. In Figure 2 we observe that for most samples the model is very certain of their label since the predicted probability is either close to $0$ or close to $1$. \n",
    "\n",
    "Consider what will happen when we transition from a continuous scale to a discrete one? Which samples are most definitly going to be classified as $0$ or $1$? For which samples is the model less certain about their class assignment?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "model = AgodaLinearCancellationEstimator().fit(train_X, train_y)\n",
    "y_prob = model.predict(test_X)\n",
    "\n",
    "go.Figure([\n",
    "    go.Scatter3d(x=test_X[:, 0], y=test_X[:, 1], z=[-0.1] * test_X.shape[0],\n",
    "                 mode='markers',\n",
    "                 marker=dict(color=test_y, symbol=\"circle-open\", colorscale=c,\n",
    "                             reversescale=True, size=1)),\n",
    "    go.Scatter3d(x=test_X[:, 0], y=test_X[:, 1], z=y_prob, mode='markers',\n",
    "                 marker=dict(color=y_prob, colorscale=custom,\n",
    "                             reversescale=True, showscale=True, size=3))],\n",
    "    layout=go.Layout(title=r\"$(2)\\text{ Predicted Class Probabilities}$\",\n",
    "                     scene_aspectmode=\"cube\", showlegend=False,\n",
    "                     scene=dict(xaxis_title=\"Feature 1\",\n",
    "                                yaxis_title=\"Feature 2\",\n",
    "                                zaxis_title=\"Probabilty of Assigning Class 1\",\n",
    "                                camera=dict(eye=dict(x=1, y=-1.8, z=.1)))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An easier visualization to use for answering the questions above is the histogram below (Figure 3). As before we see that the vast majority of samples are, by the fitted classifier, either of class $0$ or of class $1$, with fewer and fewer samples with probabilities around $0.5$ of being classified as $1$.\n",
    "\n",
    "Then, we specify a threshold probability over which we classify a sample as being of class $1$. See below how different thresholds influence what samples are above or below the threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.histogram(x=y_prob, color=train_y, nbins=50, barmode=\"overlay\",\n",
    "                   color_discrete_sequence=[c[1][1], c[0][1]],\n",
    "                   labels=dict(color=r'$\\text{True Class Assignments}$',\n",
    "                               x=r'$\\text{Probability of Assigning Class }1$'),\n",
    "                   title=r\"$(3)\\text{ Histogram of Class Assignment Probabilities}$\",\n",
    "                   height=350)\n",
    "\n",
    "frames = [go.Frame(\n",
    "    data=go.Scatter(x=[t / 10, t / 10], y=[0, 600], mode=\"lines\",\n",
    "                    line=dict(color=\"black\"), showlegend=False), traces=[2])\n",
    "          for t in range(11)]\n",
    "\n",
    "fig.add_traces(frames[0][\"data\"][0]).update(frames=frames[1:]).update_layout(updatemenus=[dict(type=\"buttons\", buttons=[AnimationButtons.play(frame_duration=1000),\n",
    "                                  AnimationButtons.pause()])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So by specifying such a threshold we partition the samples into 4 sets (suppose $1$ is the \"positive label\"):\n",
    "- True Positives: Samples of class $1$ and that are predicted to be $1$.\n",
    "- True Negatives: Samples of class $0$ and that are predicted to be $0$.\n",
    "- False Positives: Samples of class $0$ and that are predicted to be $1$.\n",
    "- False Negatives: Samples of class $1$ and that are predicted to be $0$.\n",
    "\n",
    "Confusion matrices are a common way of visualizing these four groups. It is to note that confusion matrices are also used in a wider context where we have two sets of categories $A_1,\\ldots,A_k$ and $B_1,\\ldots,B_l$ and samples are characterized by both a category of $A$ and $B$. Then, the confusion matrix will show the number of samples in each pair $A_i,B_j,\\,\\, i\\in\\left[k\\right], j\\in\\left[l\\right]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import plotly.figure_factory as ff\n",
    "model = AgodaLinearCancellationEstimator().fit(train_X, train_y)\n",
    "\n",
    "mean_prediction, prediction_ribbon_radius = \\\n",
    "    np.ndarray([91]), np.ndarray([91])\n",
    "p_predictions = np.ndarray([10])\n",
    "for p in range(10, 101):\n",
    "    for i in range(10):\n",
    "        samples = train_X.sample(frac=(p / 100))\n",
    "        labels = train_y.reindex_like(samples)\n",
    "        model.fit(samples, labels)\n",
    "        p_predictions[i] = model.loss(test_X, test_y)\n",
    "    mean_prediction[p - 10] = np.mean(p_predictions)\n",
    "    prediction_ribbon_radius[p - 10] = 2 * np.std(p_predictions)\n",
    "ps = np.array(range(10, 101))\n",
    "fig = go.Figure([go.Scatter(x=ps, y=mean_prediction, mode=\"markers+lines\",\n",
    "                            name=\"Mean Prediction\"),\n",
    "                 go.Scatter(x=ps,\n",
    "                            y=mean_prediction - prediction_ribbon_radius,\n",
    "                            fill=None, mode=\"lines\",\n",
    "                            line=dict(color=\"lightgrey\"),\n",
    "                            name=\"Confidence Interval\"),\n",
    "                 go.Scatter(x=ps,\n",
    "                            y=mean_prediction + prediction_ribbon_radius,\n",
    "                            fill='tonexty', mode=\"lines\",\n",
    "                            line=dict(color=\"lightgrey\"),\n",
    "                            showlegend=False)])\n",
    "fig.layout = go.Layout(\n",
    "    title=r\"$\\mathbb{E}(\\text{Loss})\\text{ As a Function of }p \\text{% \"\n",
    "          r\"With a Confidence Interval of } \\mathbb{E}(\\text{Loss}) \\pm\"\n",
    "          r\" 2\\cdot \\sigma_\\text{Loss}.$\",\n",
    "    xaxis_title=\"$p$\", yaxis_title=r\"$\\mathbb{E}(\\text{Loss})$\")\n",
    "fig.show()\n",
    "\n",
    "# fig = make_subplots(rows=1, cols=2,\n",
    "#                     subplot_titles=[r\"$\\text{Assigned Probability}$\",\n",
    "#                                     r\"$\\text{Confusion Matrix}$\"])\n",
    "#\n",
    "# frames = []\n",
    "# for t in range(11):\n",
    "# # Create histogram of class probabilities and mark with vertical line current threshold\n",
    "#     hist = px.histogram(x=y_prob, color=train_y, barmode=\"overlay\", nbins=10,\n",
    "#                         color_discrete_sequence=[c[1][1], c[0][1]]).add_trace(go.Scatter(x=[t / 10, t / 10], y=[0, 800], mode=\"lines\",\n",
    "#                               line=dict(color=\"black\"), showlegend=False))\n",
    "#\n",
    "# # Create a confusion matrix for classifying based on current threshold t/10\n",
    "# cm = confusion_matrix(test_y, model.predict(test_X) >= t / 10, labels=[0, 1])\n",
    "# cm = ff.create_annotated_heatmap(cm, y=[r\"$y = 0$\", r\"$y = 1$\"],\n",
    "#                                  x=[r\"$\\widehat{y} = 0$\", r\"$\\widehat{y}=1$\"],\n",
    "#                                  annotation_text=np.core.defchararray.add(\n",
    "#                                      np.array(\n",
    "#                                          [[\"TN: \", \"FP: \"], [\"FN: \", \"TP: \"]]),\n",
    "#                                      cm.astype(\"<U4\")),\n",
    "#                                  showscale=True, colorscale=\"OrRd\")\n",
    "#\n",
    "# # Create anomation frame using graphs above and their annotations\n",
    "# for annot in cm.layout.annotations: annot[\"xref\"], annot[\"yref\"] = \"x2\", \"y2\"\n",
    "# frames.append(go.Frame(data=hist.data + cm.data,\n",
    "#                        layout=go.Layout(\n",
    "#                            annotations=fig.layout.annotations + hist.layout.annotations + cm.layout.annotations),\n",
    "#                        traces=[0, 1, 2, 3]))\n",
    "#\n",
    "# # Incorporate frames' data and annotations into subplot figures\n",
    "# fig.add_traces(frames[0].data[:3], 1, 1).add_traces(frames[0].data[3], 1, 2).update(frames=frames).update_xaxes(title_text=r'$\\text{Probability of Assigning Class }1$',\n",
    "#                   row=1, col=1).update_layout(\n",
    "#     title=r\"$(4)\\text{ Confusion Matrix For Different Thresholds}$\",\n",
    "#     height=350, showlegend=False, margin=dict(t=60),\n",
    "#     annotations=frames[0].layout.annotations, barmode=\"overlay\",\n",
    "#     updatemenus=[dict(type=\"buttons\",\n",
    "#                       buttons=[AnimationButtons.play(frame_duration=1500),\n",
    "#                                AnimationButtons.pause()])])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the confusion matrix (calculated for a given threshold) we are able to define different measures of performance for the classifier. If for example, we want to ask what is the classifier's accuracy we will look at the fraction of correcly classified samples, that is, $Accuracy=TP+TN/(P+N)$.\n",
    "\n",
    "Similarly, we can define many other useful measures. Two measures that are commonly used in the field of computer science are:\n",
    "- False Positive Rate: the fraction of false-positives out of all positives: $FPR=FP/N$. This measure describes how unsuccessful the classifier is in specifying negative labels.\n",
    "- True Positive Rate (sensitivity): the fraction true-positives out of all positives: $TPR=TP/P$. This measure describes how successful the classifier is in specifying positive labels.\n",
    "\n",
    "These two measures are then combined into a single graph - the ROC curve. In this grahp, each point represents a threshold that yielded a pait of $(FPR,TPR)$. It can help us evaluate the classifiers performance in classifying positive samples and determine the optimal threshold value.\n",
    "\n",
    "Another use of the ROC curve is to calculate further summary statistics such as the Area Under Curve (AUC). Given randomly chosen positive an negative samples, the AUC equals to the probability that the classifier will output a higher probability of assignment to class $1$ for the positive sample compared to the negative sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(test_y, y_prob)\n",
    "\n",
    "go.Figure(\n",
    "    data=[go.Scatter(x=[0, 1], y=[0, 1], mode=\"lines\",\n",
    "                     line=dict(color=\"black\", dash='dash'),\n",
    "                     name=\"Random Class Assignment\"),\n",
    "          go.Scatter(x=fpr, y=tpr, mode='markers+lines', text=thresholds,\n",
    "                     name=\"\", showlegend=False, marker_size=5,\n",
    "                     marker_color=c[1][1],\n",
    "                     hovertemplate=\"<b>Threshold:</b>%{text:.3f}<br>FPR: %{x:.3f}<br>TPR: %{y:.3f}\")],\n",
    "    layout=go.Layout(\n",
    "        title=rf\"$\\text{{ROC Curve Of Fitted Model - AUC}}={auc(fpr, tpr):.6f}$\",\n",
    "        xaxis=dict(title=r\"$\\text{False Positive Rate (FPR)}$\"),\n",
    "        yaxis=dict(title=r\"$\\text{True Positive Rate (TPR)}$\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time To Think...\n",
    "\n",
    "In the above we saw how fitting a logistic regression classifier over the given data yields class assignment probabilities which then, by determining a threshold, can be converted into a binday classification. By computing the ROC curve of the fitted model we get a sense into how the model performed in terms of the prediction of positive samples. This can then be summaries further using the AUC statistic.\n",
    "\n",
    "- Consider the case seen above but this time, what will happen if the samples of the two classes are closer to each other? Modify the generated data above such that the samples of class $1$ (in red) are sampled from: $$\\mathcal{N}\\left(\\left[\\begin{array}{c} 2.5\\\\4\\end{array}\\right],\\left[\\begin{array}{cc} 1 & 0.8\\\\0.8 & 1\\end{array}\\right]\\right)$$ How does this influence the fitted model - is the classifier more or less able to classify the samples correctly? Is the AUC statistic higher or lower than before?\n",
    "- Consider the case seen above but this time, what will happen if the samples of the two classes are further apart and the variance of samples in the one class is much higher? Modify the generated data above suc hthat the samples of class $1$ are sampled from: $$\\mathcal{N}\\left(\\left[\\begin{array}{c} 4\\\\5\\end{array}\\right],\\left[\\begin{array}{cc} 5 & 3.5\\\\3.5 & 8\\end{array}\\right]\\right)$$ Compared to the setting in the previous bullet, is the classifier more confident about the probabilities it assigns? (in the sense of them being more towards $0$ and $1$). Does the classifier in this case have more misclassifications of the one type compared to the previous bullet?\n",
    "\n",
    "It is **highly recommented** to further investigate some of the different measures of performance such as [precision and specificity](https://en.wikipedia.org/wiki/Sensitivity_and_specificity). Each different measure captures a different aspect of what was done by the classifier. Different applications tend to care more for different measures. Therefore, depending on the application one might prefer different measures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
